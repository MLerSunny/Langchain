app:
  name: "Insurance Assistant"
  version: "1.0.0"
  environment: "development"
  debug: true
  log_level: "INFO"

rag:
  chunk_size: 256
  chunk_overlap: 200
  max_chunks: 10
  similarity_threshold: 0.7
  cache_results: true
  cache_ttl: 7200

llm:
  model: "deepseek-ai/deepseek-llm-7b-base"
  temperature: 0.7
  max_tokens: 4096
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0

vector_store:
  type: "chroma"
  persist_directory: "data/vector_store/chroma"
  collection_name: "documents"
  dimension: 384
  index_params:
    hnsw_ef_construction: 100
    hnsw_m: 16

embeddings:
  model: "all-MiniLM-L6-v2"
  cache_dir: "data/cache"
  batch_size: 64

security:
  api_key: "your-secret-key-here"
  allowed_origins:
    - "http://localhost:8501"
  rate_limit: 100
  max_file_size: 10485760

chunking:
  chunk_size: 256
  chunk_overlap: 200
  max_chunks: 10
  chunk_strategy: "recursive"
  min_chunk_size: 100
  max_chunk_size: 2000
  split_by: "paragraph"

retrieval:
  similarity_threshold: 0.7
  max_results: 5
  rerank_results: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_top_k: 3
  diversity_penalty: 0.1
  max_marginal_relevance: true
  mmr_lambda: 0.7

cache:
  enabled: true
  ttl: 7200
  max_size: 5000
  strategy: "lru"
  persist: true
  persist_dir: "data/cache"
  memory_limit: "80GB"

query:
  preprocess: true
  expand_queries: true
  max_query_length: 512
  query_template: "Answer the following question based on the context: {query}"
  system_prompt: "You are a helpful AI assistant that answers questions based on the provided context."

generation:
  model: "deepseek-ai/deepseek-llm-7b-base"
  temperature: 0.7
  max_tokens: 4096
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stop_sequences: []
  include_sources: true
  max_source_tokens: 2048
  use_fp16: true
  load_in_8bit: false
  device_map: "auto"
  max_memory: "80GB"

monitoring:
  enabled: true
  log_level: "INFO"
  metrics:
    - "response_time"
    - "token_usage"
    - "cache_hits"
    - "similarity_scores"
  alerts:
    high_latency:
      threshold: 2000
      action: "log"

error_handling:
  retry_attempts: 3
  retry_delay: 1000
  fallback_responses:
    timeout: "I apologize, but I'm taking too long to respond. Please try again."
    error: "I apologize, but I encountered an error. Please try again."

optimization:
  parallel_processing: true
  max_workers: 8
  batch_processing: true
  batch_size: 64
  prefetch_factor: 4
  pin_memory: true
  use_fp16: true
  gradient_checkpointing: false
  use_flash_attention: true
  use_cache: true

features:
  semantic_search: true
  hybrid_search: true
  query_expansion: true
  result_reranking: true
  source_citations: true
  answer_validation: true
  context_window: true

nlp:
  spacy_model: "en_core_web_sm"
  entity_types:
    - "ORG"
    - "PERSON"
    - "GPE"
    - "MONEY"
    - "PERCENT"
    - "DATE"
    - "TIME"
  sentiment_model: "distilbert-base-uncased-finetuned-sst-2-english"  # Example for transformers sentiment 

server:
  host: "localhost"
  port: 8000
  fastapi_port: 8000
  output_dir: "data/models" 