app:
  name: "RAG System"
  version: "1.0.0"
  environment: "development"
  config_path: "config/rag.yaml"
  log_level: "INFO"
  debug: true

rag:
  chunk_size: 256
  chunk_overlap: 200
  max_chunks: 10
  similarity_threshold: 0.7
  cache_results: true
  cache_ttl: 7200

llm:
  backend: "huggingface"  # Options: "huggingface" or "vllm"
  model: "microsoft/phi-2"
  temperature: 0.7
  max_tokens: 2048
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  repetition_penalty: 1.15
  use_8bit: false  # Disable 8-bit quantization by default
  device_map: "auto"
  max_memory: {0: "3GB"}
  local_files_only: true
  trust_remote_code: true
  offline_mode: true
  verify_ssl: false
  ssl_cert_reqs: "CERT_NONE"

ollama_llm:
  model: "deepseek-llm:7b"
  base_url: "http://localhost:11434"
  temperature: 0.7
  max_tokens: 4096
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  context_window: 4096
  stop_sequences: []
  timeout: 120
  retry_attempts: 3
  retry_delay: 2

vector_store:
  type: "chroma"
  persist_directory: "data/vector_store/chroma"
  collection_name: "documents"
  dimension: 384
  index_params:
    hnsw_ef_construction: 100
    hnsw_m: 16
  mode: "append"  # Default mode for all operations

embeddings:
  model: "sentence-transformers/all-MiniLM-L6-v2"
  device: "auto"
  trust_remote_code: true
  normalize_embeddings: true
  batch_size: 32

security:
  api_key: "your_api_key_here"
  allowed_origins:
    - "http://localhost:8501"
  rate_limit: 100
  max_file_size: 10485760

chunking:
  chunk_size: 256
  chunk_overlap: 200
  max_chunks: 10
  chunk_strategy: "recursive"
  min_chunk_size: 100
  max_chunk_size: 2000
  split_by: "paragraph"

retrieval:
  similarity_threshold: 0.7
  max_results: 5
  rerank_results: true
  rerank_model: "cross-encoder/ms-marco-MiniLM-L-6-v2"
  rerank_top_k: 3
  diversity_penalty: 0.1
  max_marginal_relevance: true
  mmr_lambda: 0.7

cache:
  enabled: true
  ttl: 7200
  max_size: 5000
  strategy: "lru"
  persist: true
  persist_dir: "data/cache"
  memory_limit: "80GB"

query:
  query_template: "Context: {context}\n\nQuestion: {query}\n\nAnswer:"
  system_prompt: "You are a helpful AI assistant that answers questions based on the provided context. Always be accurate, concise, and helpful."
  preprocess: true
  max_query_length: 512
  expand_queries: true

generation:
  model: "microsoft/phi-2"
  temperature: 0.7
  max_tokens: 2048
  top_p: 0.95
  frequency_penalty: 0.0
  presence_penalty: 0.0
  stop_sequences: []
  include_sources: true
  max_source_tokens: 1024
  use_fp16: true
  device_map: "auto"

monitoring:
  enabled: true
  log_level: "INFO"
  metrics:
    - "latency"
    - "throughput"
    - "error_rate"
    - "token_usage"

error_handling:
  retry_attempts: 3
  retry_delay: 1000
  fallback_responses:
    generation_error: "I apologize, but I encountered an error while generating a response. Please try again."
    retrieval_error: "I apologize, but I encountered an error while retrieving information. Please try again."
    validation_error: "I apologize, but I couldn't generate a satisfactory response. Please try rephrasing your question."

optimization:
  use_gpu: true
  batch_size: 64
  max_workers: 8
  use_fp16: true
  use_flash_attention: true
  tensor_parallel_size: 1
  gpu_memory_utilization: 0.9
  parallel_processing: true

features:
  enable_streaming: true
  enable_metrics: true
  enable_caching: true
  enable_validation: true

nlp:
  spacy_model: "en_core_web_sm"
  entity_types:
    - "ORG"
    - "PERSON"
    - "GPE"
    - "MONEY"
    - "PERCENT"
    - "DATE"
    - "TIME"
  sentiment_model: "distilbert-base-uncased-finetuned-sst-2-english"

server:
  host: "0.0.0.0"
  port: 8000
  fastapi_port: 8000
  streamlit_port: 8501
  output_dir: "data/models"

# ShareGPT Conversion Settings
sharegpt:
  # LLM Configuration
  ollama_llm:
    model: "deepseek-llm:7b"
    base_url: "http://localhost:11434"
    temperature: 0.7
    max_tokens: 4096
    top_p: 0.95
    frequency_penalty: 0.0
    presence_penalty: 0.0
    context_window: 4096
    stop_sequences: []
    timeout: 120
    retry_attempts: 3
    retry_delay: 2

  # Query Generation Settings
  query:
    query_template: |
      Generate {num_questions} relevant questions based on the following content.
      The questions should be specific, clear, and test understanding of key concepts.

      Content:
      {content}

      Questions:
    response_format: "newline"  # Options: newline, json, raw
    questions_per_chunk: 2
    min_question_length: 10
    max_question_length: 200

  # Error Handling Settings
  error_handling:
    use_fallback: true
    retry_attempts: 3
    retry_delay: 1000  # milliseconds
    log_level: "WARNING"
    fallback_strategy: "rule_based"  # Options: rule_based, simple, none

  # Processing Settings
  processing:
    chunk_size: 256
    chunk_overlap: 128
    max_chunks: null
    batch_size: 20
    max_workers: 4
    memory_limit: 1024  # MB

  # Output Settings
  output:
    format: "json"
    indent: 2
    ensure_ascii: false
    encoding: "utf-8"

fine_tuning:
  default_epochs: 3
  default_batch_size: 8
  default_learning_rate: 2e-5

# Paths
paths:
  data_dir: "./data"
  training_data_dir: "./data/training/data"
  models_dir: "./data/models"
  logs_dir: "./data/logs"
  cache_dir: "./data/cache"
  metrics_dir: "./data/metrics"
  jobs_dir: "./data/jobs" 